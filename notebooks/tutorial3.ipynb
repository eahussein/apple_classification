{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516bf840-42b4-4042-87a4-fe76ad285f67",
   "metadata": {},
   "source": [
    "# Tutorial 3: Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6c32e-1e9f-49d0-b2f0-ca3ccd272280",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7163a-5ed0-4b4b-a98c-9b1ba96bbcda",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6511fe-d3de-4103-b4e4-ccb5e44af55b",
   "metadata": {},
   "source": [
    "Welcome! This tutorial will show you how to pre-process features to use for classification. This is an essential pre-processing step, because the infrared spectrum data are large and highly correlated. The pre-processed features will be used to train the machine learning methods later in the upcoming tutorials.\n",
    "\n",
    "There are two general approaches to feature preprocessing. _Feature engineering_ creates new features that distill the important information needed for classification.  _Feature selection_ selects the best existing features in order to reduce random information that can confuse the classifier. Both methods serve the purpose of cutting down on redundant information as well as eliminating irrelevant information. The two methods are complementary, and often both methods are used.  In this notebook we will apply both feature engineering and feature selection to our spectral data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672c8c6",
   "metadata": {},
   "source": [
    "As usual, we begin with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b51ad-9994-4c70-a978-2685f66cd81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 1___\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70de70e-ac93-4911-b767-c207946289c2",
   "metadata": {},
   "source": [
    "Next, recall the data from the previous notebook, and display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757d274-473b-43d3-bc88-6901a04f5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 2___\n",
    "\n",
    "%store -r X\n",
    "%store -r Y\n",
    "%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edaa2b-78c9-4162-ab22-545431d496e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 3___\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05de0ad-8c82-4b27-ace2-0aaba98d18b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794cb301-e929-4b6a-a7b5-2c23a798f985",
   "metadata": {},
   "source": [
    "### Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08838ee5-fa3f-4504-86c7-34fd38fc8f07",
   "metadata": {},
   "source": [
    "We notice that consecutive columns differ very slightly, so they contain redundant information. One way to reduce the amount of data is by calculating the average of groups of ùëõ consecutive columns in a df and creating a smaller df. To illustrate this, we will demonstrate on a smaller dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0b424-4a72-442e-ae05-75b3ff1e5be6",
   "metadata": {},
   "source": [
    "##### **Example of consecutive column averaging for small dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20081b0-5d7a-467a-b3f7-c83f94bced5d",
   "metadata": {},
   "source": [
    "First let us create a dummy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c479d-14fb-4ed7-ae9f-a0bdf8c5e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 4___\n",
    "\n",
    "df_dummy = pd.DataFrame({'A': [0, 1, 2, 3, 4], 'B': [0, 10, 20, 30, 40], 'C': [0, 100, 200, 300, 400], 'D': [0, 1, 2, 3, 4],  'E': [0, 1, 2, 3, 4],  'F': [0, 1, 2, 3, 4]})\n",
    "df_dummy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baaba59-f67e-40b4-b231-9df43e3e6fab",
   "metadata": {},
   "source": [
    "Now let us take the sum of groups of 3 consecutive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b8ce2-958b-4e1b-be0a-7d67ee8b4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 5___\n",
    "\n",
    "df_dummyRoll = df_dummy.rolling(3, axis = 1).sum()  # if axis = 1, then the rolling window will go over the columns\n",
    "print(df_dummyRoll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2f08b-fbfb-4153-b02f-ea8f51fb1c30",
   "metadata": {},
   "source": [
    "The column sums overlap, but we are only interested in non-overlapping sums. So we select every third sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a296a-76eb-4d4b-a587-e08578d74755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 6___\n",
    "\n",
    "df_dummyRoll = df_dummyRoll.iloc[:, 2::3]\n",
    "df_dummyRoll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b689b81-982b-49dc-8cde-8dcdc59fcb60",
   "metadata": {},
   "source": [
    "Hopefully by now you got the idea. So let's do the same on our main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef839d-da4d-47a8-afaf-60daadad5df3",
   "metadata": {},
   "source": [
    "##### **Now applying column averaging on GS data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291b970-b3cd-49a3-bb28-45e8c0f5fec6",
   "metadata": {},
   "source": [
    "We have created a Python function in `source.utils` called `creat_rollingData`. We will be using this function to create aggregated rolling windows for a given data frame; the function has the following arguments:\n",
    " - df: the data frame\n",
    " - window_arr: an array of window sizes; we will be using this to create several aggregated datasets\n",
    " - axis: specifies the axis of aggregation (0,1 = rows, columns: the default is 1)\n",
    " - method: Name of the method used to aggregate the data. These methods have also been programmed as functions (e.g. `mean_df, skew_df, kurt_df` calculate the mean, skew, and kurtosis, respectively, of each group of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99080af0-4414-48b4-ade5-6a12a011beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 7___\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from source.utils import creat_rollingData, skew_df, mean_df, kurt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3046fd2-43d9-4676-a025-27e19aed2859",
   "metadata": {},
   "source": [
    "first let us run the `creat_rollingData` function on the dummy data we called above. We use two different window sizes (2 and 3), which produce two different feature sets. Either feature set can be used as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da24d16-4060-40eb-b77e-899efdc4428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 8___\n",
    "\n",
    "df_arrayRol_dummy  = creat_rollingData (df = df_dummy, window_arr = [2,3], method =  mean_df)\n",
    "df_arrayRol_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb30e2-b43c-499d-8b9c-b35eb0efcdcc",
   "metadata": {},
   "source": [
    "We have checked that the method is working given the above results. For the following, we create four different sets of engineered features using four different windows. Later we will compare the performance obtained using these four different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c28e60-1f89-4517-a221-e07fd1ddaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 9___\n",
    "\n",
    "X_arrayRol  = creat_rollingData (df = X, window_arr = [10, 30, 50, 100], method =  mean_df )\n",
    "\n",
    "# let us see the shape of the created rolled dataframes\n",
    "for x in X_arrayRol:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520ecb9-f501-41bf-bfe1-29d55dd7c966",
   "metadata": {},
   "source": [
    "Let us now visualise the data, we will make use of a customized function called `graph_df` in `source/graphs` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10066754-7d9e-4bd9-9766-af237085188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 10___\n",
    "\n",
    "from source.graphs import graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b53cd-22dc-4982-9a2c-974e535c297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 11___\n",
    "\n",
    "graph_df (X_arrayRol, Y, n = 50) # where n is the number of randomly selected samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840c03b-8ff2-4fb9-b663-6a3f127cc776",
   "metadata": {},
   "source": [
    "Notice, that the resolution is decreasing since we are losing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ab479-e7f6-43b8-82f2-6e3bc0703e1a",
   "metadata": {},
   "source": [
    "**Exercise 1:** Apply column averaging on the other 2 datasets\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d7a03-0b95-4a53-841e-96af1266dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ___ code here ____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05f534-515e-45d5-9ae3-9aa05a4788cd",
   "metadata": {},
   "source": [
    "**Exercise 2:** Using the function 'mean_df' as an example, write another function to compute the variance of the column groups.  Call your function `var_df`.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7fa71-1ff7-4f5f-aba9-0b931d9e2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ___ code here ____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e48ab-f931-4c7d-af41-4c11de32b72a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32035f-bf0d-4d09-ae7d-befb0a41540d",
   "metadata": {},
   "source": [
    "###  Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc52d29-6cc3-47a9-a9de-48d44adcae78",
   "metadata": {},
   "source": [
    "The idea is that we want to select the best N features from each data set we created above. I will set N = 10 (you are encouraged to change N)\n",
    "\n",
    "For feature selection, we will make use of `Sequential feature selection` (SFS) from `sklearn`. For more information about the method, visit the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d69cb0-721d-461c-ac83-1456fd8f0f06",
   "metadata": {},
   "source": [
    "We need to consider the following when using SFS:\n",
    "   -  SFS is an optimization tool; hence we will need to split the data into training and testing and perform SFS/feature selection on the training set\n",
    "   - SFS removes features based on a _cross-validation score_. To obtain this score, we will need to specify a ML model to that is run on different feature sets.  For more information about cross validation, please visit the following [link](https://scikit-learn.org/stable/modules/cross_validation.html). \n",
    "   \n",
    "   In this tutorial, we will be using logistic regression. As mentioned in a previous tutorial, we are interested in the precision of the classifier, so the cross-validation score will be based on the precision of the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb2a3f-90d8-4400-98ef-29c5f3e6a09f",
   "metadata": {},
   "source": [
    "`Notice`: It is *essential* to perform optimizations on the training set instead of the testing to avoid information leakage. Otherwise, model performances will be overestimated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85177e9c",
   "metadata": {},
   "source": [
    "We begin with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6e1a4-8ad1-4479-be2e-58b5485f3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 12___\n",
    "\n",
    "from source.utils import split #  a pre-defined function to split the data into training and testing\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd6ddc-82d7-415a-890c-2510887bd75e",
   "metadata": {},
   "source": [
    "Before We perfom feature selection, let's convert the `Y` values from `S` and `B` to `1` and `0` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7361dec-3dd1-4a0f-9388-fadac1cf75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 13___\n",
    "\n",
    "Y = Y.map({'S': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a11115-e390-4359-85eb-e52612dddd01",
   "metadata": {},
   "source": [
    "Now let us perfom the feature selection, using the methods from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8a373-aea9-4c10-b6cb-97a892fcde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 14___\n",
    "\n",
    "selected_indexes = []\n",
    "\n",
    "for x_roll in X_arrayRol:\n",
    "    Xtrain, Xtest, Ytrain, Ytest  = split( x_roll, Y) # splitting the data\n",
    "    print(\"(Number of samples, number of features) = \", Xtrain.shape)\n",
    "    sfs = SequentialFeatureSelector(estimator=LogisticRegression(solver = 'newton-cg'), n_features_to_select=10, direction = 'forward', scoring = 'precision' ) # def SFS\n",
    "    sfs.fit(Xtrain.values, Ytrain)\n",
    "    selected_indexes.append(sfs.support_) # storing the indices of the best n_features_to_select columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91b671-11c1-4ac0-99df-d3ce9594d99b",
   "metadata": {},
   "source": [
    "If you want to know how to choose the best N features automatically based on their score, you can visit this [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html), or this [link](https://machinelearningmastery.com/rfe-feature-selection-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad1eed-f203-47ac-b187-95682ee8b30d",
   "metadata": {},
   "source": [
    "**Exercise 3:** Perform feature selection on the other 2 data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731438f4-af0e-498f-b5f1-6edffef0d69e",
   "metadata": {},
   "source": [
    "**Exercise 4:** Visualise the feature importance using a scatter plot on the 3 datasets, as we would like to know where are the best 10 features on the spectrum.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34831f-7865-488f-9897-e2daa096de90",
   "metadata": {},
   "source": [
    "**DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def1411-63c1-4414-9392-7ca0716edcc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48281720-967e-4488-b776-6cff2105a37a",
   "metadata": {},
   "source": [
    "<b><i> Saving data for later use </i></b>\n",
    "\n",
    "We can save the data so that we can call it up again in the next notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd1ff1-96e2-4e89-bcc6-30504ac59d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 15___\n",
    "\n",
    "%store  X_arrayRol\n",
    "%store  selected_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def4e95-1134-42b2-8e1e-65d731c184cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
